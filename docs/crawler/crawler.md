# üï∑Ô∏è Crawler ‚Äî Documenta√ß√£o Viva (GDE Mobile ¬∑ MC656-2025-s2)

> **Escopo:** este documento lista **o que j√° foi feito** e **o que falta** no m√≥dulo *crawler*, no formato de **checklists hier√°rquicas** (listas, sublistas, sub‚Äësublistas), para orientar o desenvolvimento cont√≠nuo do projeto (n√£o √© um artefato de A3, e sim do **projeto**).

---

## 1) Objetivo e Resultado Esperado
- [x] **Objetivo do crawler**
  - [x] Coletar dados do GDE (√°rvore/disciplinas/ofertas) com **login real** e **sess√£o autenticada**.
  - [x] **Limpar e normalizar** dados para consumo pelo backend/app.
  - [x] **Persistir em JSON** (snapshots) e permitir **seed do banco**.
- [ ] **Resultado final desejado**
  - [ ] Banco geral (seed√°vel) cobrindo **~15 anos de cat√°logos**.
    - [x] Estrutura inicial de DB simples (`simple_db.py`, `simple_schema.sql`).
    - [ ] Conte√∫do completo com **semestre recomendado** por disciplina (ap√≥s troca de getter).
    - [ ] Consolida√ß√£o e deduplica√ß√£o multi‚Äëano.

---

## 2) Arquitetura Atual (vis√£o de c√≥digo)
- [x] **Estrutura de diret√≥rios (existente no zip)**
  - [x] `crawler/readme.md` (orienta√ß√µes iniciais de ambiente)
  - [x] `crawler/run_all_courses.sh` e `crawler/scripts/run_all.ps1` (execu√ß√£o)
  - [x] `crawler/template.env` (exemplo de vari√°veis)
  - [x] `crawler/src/crawler_app/`
    - [x] `__init__.py` e `cli.py` (CLI base)
    - [x] `collectors/`
      - [x] `arvore_http.py` (requisi√ß√µes HTTP √† √°rvore)
      - [x] `config.py` (BASE + *targets*)
      - [x] `enumerate_dimensions.py` (runner para coleta parametrizada)
      - [x] `enumerate_pipeline.py` (pipeline de varredura e grava√ß√£o RAW)
    - [x] `parsers/`
      - [x] `arvore_parsers.py` (**BeautifulSoup** para extrair selects/itens)
    - [x] `db/`
      - [x] `simple_db.py` (escrita/seed simples)
      - [x] `simple_schema.sql` (schema inicial)
    - [x] `tools/`
      - [x] `build_simple_db.py` (ferramenta de constru√ß√£o)
    - [x] `utils/`
      - [x] `hashing.py` (hashes de conte√∫do)
      - [x] `http_session.py` (**create_session**, **ensure_csrf_cookie**, **login_via_ajax**)
      - [x] `io_raw.py` (estrutura de pastas RAW/WRITE)
      - [x] `logging_helpers.py` (**log_response_with_selects**, print de cookies)
- [ ] **Pr√≥ximas adi√ß√µes de arquitetura**
  - [ ] `clients/gde_api.py` (clientes tipados para endpoints JSON)
  - [ ] `parse/normalizers.py` (camada de normaliza√ß√£o Pydantic)
  - [ ] `validate/` (regras de qualidade de dados e diffs)
  - [ ] `consolidate/` (merges multi‚Äëano e deduplica√ß√£o)
  - [ ] `export/` (artefatos para seed do backend)

---

## 3) Setup, Autentica√ß√£o e Sess√£o
- [x] **Ambiente e vari√°veis**
  - [x] `.env` (modelo em `template.env` / instru√ß√µes em `readme.md`)
  - [x] Leitura de credenciais (RA/senha)
- [x] **Sess√£o HTTP**
  - [x] `create_session()` com headers consistentes (UA, accept, etc.)
  - [x] `ensure_csrf_cookie()` (pr√©‚Äërequisitos do login)
  - [x] `login_via_ajax()` (login real, sess√£o com token)
- [ ] **Fortalecimentos pendentes**
  - [ ] Renova√ß√£o de token (*refresh*) autom√°tica
    - [ ] Checagem de validade a cada *N* requisi√ß√µes
    - [ ] *Retry/backoff* em 401/403
  - [ ] Sanitiza√ß√£o de logs
    - [ ] Nunca logar RA/senha/token/cookies
    - [ ] *Redactions* consistentes nos arquivos RAW/DEBUG

---

## 4) Coleta (Collectors) ‚Äî Estado Atual vs Pend√™ncias
### 4.1 Coleta de √Årvore/Seletores (HTML)
- [x] **Fetch de p√°ginas e fragments** (`arvore_http.py`)
  - [x] `/arvore/` (GET com par√¢metros)
  - [x] Fragmento de ‚Äúmodalidades‚Äù (XHR/HTML)
  - [x] *Polite sleep* para reduzir carga
- [x] **Pipeline de varredura** (`enumerate_pipeline.py` ‚Üí `enumerate_dimensions.py`)
  - [x] Limpeza e recria√ß√£o de pasta RAW por execu√ß√£o
  - [x] *Selects* capturados e serializados
  - [x] Itera√ß√£o parametriz√°vel por **curso/catalogo/per√≠odo** via `config.py`
- [ ] **Pend√™ncias** (HTML ‚Üí JSON endpoints)
  - [ ] Migrar para **text‚Äëscraper/endpoint‚Äëscraper** (capturar JSON do front)
    - [ ] Mapear endpoints e *query params* no `docs/gde_endpoints.md`
    - [ ] Reproduzir chamadas XHR com mesma sess√£o/autoriza√ß√£o
    - [ ] Diminuir depend√™ncia de parsing HTML e *fragility* de seletores

### 4.2 Coleta de Disciplinas/Ofertas/Professores
- [x] **Disciplinas obrigat√≥rias/eletivas** (estado atual)
  - [x] Extra√≠das da √°rvore/fragmentos
- [x] **Pr√©/co‚Äërequisitos** (estrutura de integraliza√ß√£o)
  - [x] Grafo: n√≥s (disciplinas) e arestas (rela√ß√£o)
- [ ] **Semestre recomendado**
  - [ ] Incluir campo `recommended_semester` (ap√≥s troca de getter)
  - [ ] Validar coer√™ncia por *curso/catalogo*

### 4.3 Coleta Multi‚ÄëAno (15 anos)
- [ ] **Loop parametrizado por `catalogo`**
  - [ ] Orquestrador `run_year(year)` usando **mesma sess√£o** (sem relogar)
  - [ ] `run_all_years(start=YYYY, count=15)` com:
    - [ ] *Rate limit* configur√°vel (QPS)
    - [ ] *Retry/backoff* (falhas transit√≥rias)
    - [ ] *Cache* local opcional (evitar requisi√ß√µes id√™nticas)
  - [ ] Relat√≥rio por ano (contagens de cursos/ofertas/arestas)

---

## 5) Parsing e Normaliza√ß√£o (Camada de Dom√≠nio)
- [x] **Parsing atual (HTML)** (`parsers/arvore_parsers.py`)
  - [x] Extra√ß√£o de `<select>` e `<option>` com **BeautifulSoup**
  - [x] Tratamento de valores e r√≥tulos (limpeza b√°sica)
- [ ] **Normaliza√ß√£o Pydantic (pendente)**
  - [ ] `Course{ code, name, credits, type }`
    - [ ] `type ‚àà {obrigatoria,eletiva,livre}` (normalizado)
    - [ ] `credits > 0`
  - [ ] `Offer{ course_code, term, class_id, schedule:[ScheduleSlot], teacher }`
    - [ ] `ScheduleSlot{ day ‚àà {seg,...,sab}, start,end,room }`
    - [ ] Regras de interse√ß√£o/confian√ßa para conflitos de hor√°rio
  - [ ] `CurriculumNode{ course_code, category, recommended_semester? }`
    - [ ] `Edge{ from, to, type ‚àà {prereq,coreq} }`
  - [ ] **Normalizers**: HTML/JSON ‚Üí modelos tipados
    - [ ] *Coercion* de tipos (int/str/enum) e *defaults* consistentes
    - [ ] Fun√ß√µes puras com testes

---

## 6) Persist√™ncia, Snapshots e Consolida√ß√£o
- [x] **RAW** (provas/refer√™ncias)
  - [x] Gravar respostas *in natura* (HTML/JSON) em `data/raw/{year}/‚Ä¶`
  - [x] Nomear por endpoint/par√¢metros (reprodutibilidade)
- [ ] **CLEAN** (dados normalizados)
  - [ ] `data/clean/{year}/{courses,offers,curriculum}.json`
  - [ ] `data/clean/all/{courses,offers,curriculum}.json` (consolidados)
  - [ ] **Consolidadores**
    - [ ] `consolidate_year(year)` (dedup por chave natural)
    - [ ] `consolidate_all()` (merge multi‚Äëano, *conflict resolution*)
- [ ] **Hashes e integridade**
  - [ ] Calcular **sha256** por arquivo limpo
  - [ ] Gravar *manifest* com contagens e *hashes*

---

## 7) Qualidade de Dados (DQ) e Diffs
- [ ] **Valida√ß√µes de regra de neg√≥cio**
  - [ ] Cr√©ditos positivos (>= 1)
  - [ ] Hor√°rios v√°lidos (intervalos e sobreposi√ß√µes)
  - [ ] *Edges* v√°lidos (sem *loops* imposs√≠veis; *from*/*to* existentes)
- [ ] **Valida√ß√µes de preenchimento**
  - [ ] Campos obrigat√≥rios n√£o‚Äënulos
  - [ ] Valores em dom√≠nios permitidos (enums)
- [ ] **Diffs entre execu√ß√µes**
  - [ ] Comparar `clean/{year}` com run anterior
    - [ ] Disciplinas adicionadas/removidas/alteradas
    - [ ] Ofertas com hor√°rio/docente alterado
  - [ ] Gerar `reports/diff-YYYYMMDD.md`

---

## 8) Export para o Backend (Seeds/Fixtures)
- [x] **DB simples** (`db/simple_db.py`, `db/simple_schema.sql`)
  - [x] Ferramenta `tools/build_simple_db.py`
- [ ] **Exportadores**
  - [ ] *CSV/JSON* em formato esperado pelo backend
  - [ ] *Script* `seed_from_json.py` (ou endpoint no backend para ingest√£o)
  - [ ] Verifica√ß√£o de chaves estrangeiras e ordinalidade

---

## 9) CLI de Opera√ß√£o (unificada)
- [x] **CLI base** (`cli.py` presente)
- [ ] **Comandos finais**
  - [ ] `healthcheck` (login + chamada simples autenticada)
  - [ ] `run_year --year YYYY`
  - [ ] `run_all_years --start YYYY --count 15`
  - [ ] `validate` (DQ + integridade)
  - [ ] `consolidate_all`
  - [ ] `export --format csv|json`

---

## 10) Observabilidade, Resili√™ncia e √âtica de Coleta
- [x] **Logs b√°sicos** (`utils/logging_helpers.py`)
- [ ] **Melhorias de observabilidade**
  - [ ] Log estruturado (n√≠vel, ctx, request_id)
  - [ ] Sum√°rio por execu√ß√£o (contagens/tempos/erros)
  - [ ] *Tracing* por ano/endpoint
- [ ] **Resili√™ncia**
  - [ ] *Retry* com *backoff exponencial*
  - [ ] *Circuit breaker* para timeouts seguidos
  - [ ] Ajuste de *polite_sleep* por endpoint
- [ ] **√âtica e conformidade**
  - [ ] *Rate limit* configur√°vel
  - [ ] Respeito a *robots*/ToS quando aplic√°vel
  - [ ] Execu√ß√µes fora do hor√°rio de pico (se necess√°rio)

---

## 11) Migra√ß√£o para Text‚ÄëScraper / Endpoints JSON (DETALHADO)
- [ ] **Descoberta de endpoints (DevTools ‚Üí Network)**
  - [ ] Identificar rotas de:
    - [ ] Listagem de cursos
    - [ ] Ofertas por per√≠odo
    - [ ] Estrutura de √°rvore/pr√©/co‚Äëreq
    - [ ] Metadados de **semestre recomendado**
  - [ ] Anotar:
    - [ ] M√©todo (GET/POST), *headers*, *cookies*, *CSRF*
    - [ ] Par√¢metros de *query* e *payload*
    - [ ] Exemplos de resposta (salvar *fixtures*)
- [ ] **Clientes tipados (`clients/gde_api.py`)**
  - [ ] `list_courses(year)`
  - [ ] `list_offers(year, term)`
  - [ ] `get_curriculum(course_id, year)`
  - [ ] `get_prereqs(course_id, year)`
  - [ ] `get_semester_map(course_id, year)`
- [ ] **Integra√ß√£o com sess√£o existente**
  - [ ] Reaproveitar token/cookies do login atual
  - [ ] Replicar *headers* do front (UA, XHR, CSRF quando houver)
- [ ] **Paridade com vers√£o HTML**
  - [ ] *Parity tests*: JSON vs HTML para o mesmo **curso/ano**
  - [ ] Definir *fallback* para campos ausentes no JSON

---

## 12) Backlog T√©cnico (prioridade por valor)
- [ ] **(P1)** Adicionar **semestre recomendado** na normaliza√ß√£o (destrava E2 no app).
- [ ] **(P1)** Implementar **loop multi‚Äëano (15 anos)** com relat√≥rio por ano.
- [ ] **(P1)** Migrar coleta principal para **endpoints JSON** (robustez/velocidade).
- [ ] **(P2)** Validadores e **diffs** (qualidade e rastreabilidade).
- [ ] **(P2)** Exportadores e **seed** plug‚Äëand‚Äëplay no backend.
- [ ] **(P3)** Observabilidade avan√ßada (tracing, m√©tricas, circuit breaker).

---

## 13) Comandos Recomendados (ao final desta fase)
```bash
# Sanidade de login/sess√£o
python -m crawler_app.cli healthcheck

# Coleta por cat√°logo (reaproveita token)
python -m crawler_app.cli run_year --year 2025

# Coleta de ~15 anos (com rate limit e retry)
python -m crawler_app.cli run_all_years --start 2025 --count 15

# Consolida√ß√£o + valida√ß√µes
python -m crawler_app.cli consolidate_all
python -m crawler_app.cli validate

# Export para backend
python -m crawler_app.cli export --format json
```

---

## 14) Anexos e Refer√™ncias (no reposit√≥rio)
- `crawler/readme.md` (setup de ambiente)
- `crawler/template.env` (vari√°veis)
- `crawler/src/crawler_app/utils/http_session.py` (sess√£o/login/csrf)
- `crawler/src/crawler_app/collectors/*.py` (coleta atual HTML)
- `crawler/src/crawler_app/parsers/arvore_parsers.py` (parsing HTML)
- `crawler/src/crawler_app/db/*` (DB simples, schema e builder)
- `crawler/src/crawler_app/tools/build_simple_db.py` (ferramenta)
